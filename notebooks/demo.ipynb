{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1TENTH Dashcam to Costmap Translation - Demo\n",
    "\n",
    "This notebook demonstrates the capabilities of our image-to-image translation models for converting F1TENTH dashcam images to navigation costmaps.\n",
    "\n",
    "## Models Overview\n",
    "\n",
    "We implement two different neural network architectures:\n",
    "\n",
    "1. **UNet150**: A U-Net architecture optimized for 150x150 pixel images\n",
    "2. **ContextNetwork**: A dilated convolution network for capturing multi-scale context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import UNet150, ContextNetwork\n",
    "from data import ImageToImageDataset, EnhancedImageToImageDataset\n",
    "from utils import get_device, load_model\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "Let's first examine our dataset to understand the input-output relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_FOLDER = \"../Data/Dashcams\"\n",
    "TARGET_FOLDER = \"../Data/Costmaps\"\n",
    "device = get_device()\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Basic transforms\n",
    "transform_input = transforms.Compose([transforms.ToTensor()])\n",
    "transform_target = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Create dataset\n",
    "dataset = ImageToImageDataset(\n",
    "    input_folder=INPUT_FOLDER,\n",
    "    target_folder=TARGET_FOLDER,\n",
    "    transform_input=transform_input,\n",
    "    transform_target=transform_target,\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample data\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "batch_inputs, batch_targets = next(iter(data_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(4):\n",
    "    # Input dashcam images\n",
    "    axes[0, i].imshow(batch_inputs[i].squeeze(), cmap=\"gray\")\n",
    "    axes[0, i].set_title(f\"Dashcam Input {i+1}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    # Target costmaps\n",
    "    axes[1, i].imshow(batch_targets[i].squeeze(), cmap=\"gray\")\n",
    "    axes[1, i].set_title(f\"Costmap Target {i+1}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Dataset Examples: Dashcam Images â†’ Costmaps\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: U-Net Architecture\n",
    "\n",
    "The U-Net model uses an encoder-decoder architecture with skip connections to preserve spatial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load U-Net model\n",
    "unet_model = UNet150(in_channels=1, out_channels=1, complexity_multiplier=4)\n",
    "\n",
    "# Try to load pre-trained weights\n",
    "unet_model_path = \"../models/unet_best.pth\"\n",
    "if os.path.exists(unet_model_path):\n",
    "    unet_model = load_model(unet_model, unet_model_path, device)\n",
    "    print(\"Loaded pre-trained U-Net model\")\n",
    "else:\n",
    "    print(\"No pre-trained U-Net found, using random weights\")\n",
    "\n",
    "unet_model.to(device)\n",
    "unet_model.eval()\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in unet_model.parameters())\n",
    "print(f\"U-Net total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate U-Net predictions\n",
    "with torch.no_grad():\n",
    "    batch_inputs_device = batch_inputs.to(device)\n",
    "    unet_predictions = unet_model(batch_inputs_device).cpu()\n",
    "\n",
    "# Visualize U-Net results\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(4):\n",
    "    # Input\n",
    "    axes[0, i].imshow(batch_inputs[i].squeeze(), cmap=\"gray\")\n",
    "    axes[0, i].set_title(f\"Input {i+1}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    # Ground truth\n",
    "    axes[1, i].imshow(batch_targets[i].squeeze(), cmap=\"gray\")\n",
    "    axes[1, i].set_title(f\"Ground Truth {i+1}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "    # U-Net prediction\n",
    "    axes[2, i].imshow(unet_predictions[i].squeeze(), cmap=\"gray\")\n",
    "    axes[2, i].set_title(f\"U-Net Prediction {i+1}\")\n",
    "    axes[2, i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"U-Net Model Results\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Context Network\n",
    "\n",
    "The Context Network uses dilated convolutions to capture multi-scale contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced dataset for Context Network\n",
    "MEAN = 0.2335\n",
    "STD = 0.1712\n",
    "\n",
    "transform_input_context = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_target_context = transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "enhanced_dataset = EnhancedImageToImageDataset(\n",
    "    input_folder=INPUT_FOLDER,\n",
    "    target_folder=TARGET_FOLDER,\n",
    "    transform_input=transform_input_context,\n",
    "    transform_target=transform_target_context,\n",
    "    threshold=150,\n",
    "    filter_size=50,\n",
    ")\n",
    "\n",
    "enhanced_loader = DataLoader(enhanced_dataset, batch_size=4, shuffle=True)\n",
    "batch_inputs_enh, batch_targets_enh, masks = next(iter(enhanced_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Context Network\n",
    "context_model = ContextNetwork(\n",
    "    in_channels=1, out_channels=1, dilation_factors=[1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    ")\n",
    "\n",
    "# Try to load pre-trained weights\n",
    "context_model_path = \"../models/context_net_best.pth\"\n",
    "if os.path.exists(context_model_path):\n",
    "    context_model = load_model(context_model, context_model_path, device)\n",
    "    print(\"Loaded pre-trained Context Network model\")\n",
    "else:\n",
    "    print(\"No pre-trained Context Network found, using random weights\")\n",
    "\n",
    "context_model.to(device)\n",
    "context_model.eval()\n",
    "\n",
    "# Model summary\n",
    "total_params_context = sum(p.numel() for p in context_model.parameters())\n",
    "print(f\"Context Network total parameters: {total_params_context:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Context Network predictions\n",
    "with torch.no_grad():\n",
    "    batch_inputs_enh_device = batch_inputs_enh.to(device)\n",
    "    context_predictions = context_model(batch_inputs_enh_device).cpu()\n",
    "\n",
    "# Denormalize inputs for visualization\n",
    "batch_inputs_vis = batch_inputs_enh * STD + MEAN\n",
    "\n",
    "# Visualize Context Network results\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "\n",
    "for i in range(4):\n",
    "    # Input (denormalized)\n",
    "    axes[0, i].imshow(batch_inputs_vis[i].squeeze(), cmap=\"gray\")\n",
    "    axes[0, i].set_title(f\"Input {i+1}\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "\n",
    "    # Ground truth\n",
    "    axes[1, i].imshow(batch_targets_enh[i].squeeze(), cmap=\"gray\")\n",
    "    axes[1, i].set_title(f\"Ground Truth {i+1}\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "\n",
    "    # Context Network prediction\n",
    "    axes[2, i].imshow(context_predictions[i].squeeze(), cmap=\"gray\")\n",
    "    axes[2, i].set_title(f\"Context Net Prediction {i+1}\")\n",
    "    axes[2, i].axis(\"off\")\n",
    "\n",
    "    # Morphological mask\n",
    "    axes[3, i].imshow(masks[i].squeeze(), cmap=\"binary\")\n",
    "    axes[3, i].set_title(f\"Morphological Mask {i+1}\")\n",
    "    axes[3, i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Context Network Model Results\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's compare both models side by side on the same input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same input for both models (first sample)\n",
    "sample_input = batch_inputs[0:1]  # Keep batch dimension\n",
    "sample_target = batch_targets[0:1]\n",
    "\n",
    "# Get predictions from both models\n",
    "with torch.no_grad():\n",
    "    # U-Net prediction\n",
    "    unet_pred = unet_model(sample_input.to(device)).cpu()\n",
    "\n",
    "    # Context Network prediction (need to normalize input)\n",
    "    sample_input_norm = (sample_input - MEAN) / STD\n",
    "    context_pred = context_model(sample_input_norm.to(device)).cpu()\n",
    "\n",
    "# Comparison visualization\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(sample_input[0].squeeze(), cmap=\"gray\")\n",
    "axes[0].set_title(\"Dashcam Input\", fontsize=14)\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(sample_target[0].squeeze(), cmap=\"gray\")\n",
    "axes[1].set_title(\"Ground Truth Costmap\", fontsize=14)\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(unet_pred[0].squeeze(), cmap=\"gray\")\n",
    "axes[2].set_title(\"U-Net Prediction\", fontsize=14)\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "axes[3].imshow(context_pred[0].squeeze(), cmap=\"gray\")\n",
    "axes[3].set_title(\"Context Network Prediction\", fontsize=14)\n",
    "axes[3].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Model Comparison\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics\n",
    "\n",
    "Let's calculate some basic metrics to compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def calculate_metrics(predictions, targets):\n",
    "    \"\"\"Calculate L1 and L2 losses.\"\"\"\n",
    "    l1_loss = F.l1_loss(predictions, targets)\n",
    "    l2_loss = F.mse_loss(predictions, targets)\n",
    "    return l1_loss.item(), l2_loss.item()\n",
    "\n",
    "\n",
    "# Calculate metrics for both models\n",
    "unet_l1, unet_l2 = calculate_metrics(unet_pred, sample_target)\n",
    "context_l1, context_l2 = calculate_metrics(context_pred, sample_target)\n",
    "\n",
    "print(\"Performance Comparison (Single Sample):\")\n",
    "print(f\"{'Model':<15} {'L1 Loss':<10} {'L2 Loss':<10} {'Parameters':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'U-Net':<15} {unet_l1:<10.4f} {unet_l2:<10.4f} {total_params:<12,}\")\n",
    "print(\n",
    "    f\"{'Context Net':<15} {context_l1:<10.4f} {context_l2:<10.4f} {total_params_context:<12,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This demo showcases two different approaches to the F1TENTH dashcam-to-costmap translation problem:\n",
    "\n",
    "1. **U-Net**: Classical encoder-decoder architecture with skip connections\n",
    "2. **Context Network**: Dilated convolutions for multi-scale context capture\n",
    "\n",
    "Both models demonstrate the ability to transform dashcam images into navigation costmaps, enabling autonomous navigation in the F1TENTH racing environment.\n",
    "\n",
    "### Key Features:\n",
    "- **150x150 pixel resolution** optimized for real-time performance\n",
    "- **Identity initialization** for Context Network stability\n",
    "- **Morphological preprocessing** for enhanced training data\n",
    "- **L1 loss optimization** for sharp costmap boundaries\n",
    "\n",
    "The models can be further improved through:\n",
    "- Extended training epochs\n",
    "- Data augmentation techniques\n",
    "- Advanced loss functions\n",
    "- Ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
